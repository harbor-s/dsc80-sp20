{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snapchat Political Ads\n",
    "This project uses political ads data from Snapchat, a popular social media app. Interesting questions to consider include:\n",
    "- What are the most prevalent organizations, advertisers, and ballot candidates in the data? Do you recognize any?\n",
    "- What are the characteristics of ads with a large reach, i.e., many views? What may a campaign consider when maximizing an ad's reach?\n",
    "- What are the characteristics of ads with a smaller reach, i.e., less views? Aside from funding constraints, why might a campaign want to produce an ad with a smaller but more targeted reach?\n",
    "- What are the characteristics of the most expensive ads? If a campaign is limited on advertising funds, what type of ad may the campaign consider?\n",
    "- What groups or regions are targeted frequently? (For example, for single-gender campaigns, are men or women targeted more frequently?) What groups or regions are targeted less frequently? Why? Does this depend on the type of campaign?\n",
    "- Have the characteristics of ads changed over time (e.g. over the past year)?\n",
    "- When is the most common local time of day for an ad's start date? What about the most common day of week? (Make sure to account for time zones for both questions.)\n",
    "\n",
    "### Getting the Data\n",
    "The data and its corresponding data dictionary is downloadable [here](https://www.snap.com/en-US/political-ads/). Download both the 2018 CSV and the 2019 CSV. \n",
    "\n",
    "The CSVs have the same filename; rename the CSVs as needed.\n",
    "\n",
    "Note that the CSVs have the exact same columns and the exact same data dictionaries (`readme.txt`).\n",
    "\n",
    "### Cleaning and EDA\n",
    "- Concatenate the 2018 CSV and the 2019 CSV into one DataFrame so that we have data from both years.\n",
    "- Clean the data.\n",
    "    - Convert `StartDate` and `EndDate` into datetime. Make sure the datetimes are in the correct time zone. You can use whatever timezone (e.g. UTC) you want as long as you are consistent. However, if you want to answer a question like \"When is the most common local time of day for an ad's start date,\" you will need to convert timezones as needed. See Hint 2 below for more information.\n",
    "- Understand the data in ways relevant to your question using univariate and bivariate analysis of the data as well as aggregations.\n",
    "\n",
    "*Hint 1: What is the \"Z\" at the end of each timestamp?*\n",
    "\n",
    "*Hint 2: `pd.to_datetime` will be useful here. `Series.dt.tz_convert` will be useful if a change in time zone is needed.*\n",
    "\n",
    "*Tip: To visualize geospatial data, consider [Folium](https://python-visualization.github.io/folium/) or another geospatial plotting library.*\n",
    "\n",
    "### Assessment of Missingness\n",
    "Many columns which have `NaN` values may not actually have missing data. How come? In some cases, a null or empty value corresponds to an actual, meaningful value. For example, `readme.txt` states the following about `Gender`:\n",
    "\n",
    ">  Gender - Gender targeting criteria used in the Ad. If empty, then it is targeting all genders\n",
    "\n",
    "In this scenario, an empty `Gender` value (which is read in as `NaN` in pandas) corresponds to \"all genders\".\n",
    "\n",
    "- Refer to the data dictionary to determine which columns do **not** belong to the scenario above. Assess the missingness of one of these columns.\n",
    "\n",
    "### Hypothesis Test / Permutation Test\n",
    "Find a hypothesis test or permutation test to perform. You can use the questions at the top of the notebook for inspiration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Findings\n",
    "\n",
    "### Introduction\n",
    "TODO\n",
    "\n",
    "### Cleaning and EDA\n",
    "TODO\n",
    "\n",
    "### Assessment of Missingness\n",
    "TODO\n",
    "\n",
    "### Hypothesis Test\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.652554Z",
     "start_time": "2019-10-31T23:36:27.180520Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'  # Higher resolution figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the CSVs\n",
    "\n",
    "pol18 = pd.read_csv('2018PoliticalAds.csv')\n",
    "pol19 = pd.read_csv('2019PoliticalAds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, I check to see if all the columns match between the CSVs\n",
    "\n",
    "pol19.columns == pol18.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I check to see if the concatenation of the CSVs was successful\n",
    "\n",
    "pol_comb = pd.concat([pol18, pol19], ignore_index = True)\n",
    "pol_comb.columns == pol19.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I chose the columns that were relevant to my question\n",
    "\n",
    "useful_cols = ['ADID', 'Spend', 'StartDate', 'EndDate', 'OrganizationName', 'CountryCode']\n",
    "pol_comb = pol_comb[useful_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>null %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>EndDate</td>\n",
       "      <td>object</td>\n",
       "      <td>0.182052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ADID</td>\n",
       "      <td>object</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>CountryCode</td>\n",
       "      <td>object</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>OrganizationName</td>\n",
       "      <td>object</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Spend</td>\n",
       "      <td>int64</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>StartDate</td>\n",
       "      <td>object</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    type    null %\n",
       "EndDate           object  0.182052\n",
       "ADID              object  0.000000\n",
       "CountryCode       object  0.000000\n",
       "OrganizationName  object  0.000000\n",
       "Spend              int64  0.000000\n",
       "StartDate         object  0.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I create a table that contains the percentage of null values in each category - everything has some value except for EndDate\n",
    "\n",
    "s_type = pol_comb.dtypes\n",
    "s_null = pol_comb.isnull().mean().sort_values(ascending = False)\n",
    "type_null = pd.concat([s_type, s_null], axis = 1)\n",
    "type_null.columns = ['type', 'null %']\n",
    "type_null.sort_values(by = 'null %', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'us_pol_comb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c733a00c9b9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# It is difficult to see the distribution because there are outliers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mus_pol_comb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Spend'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'hist'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Distribution of Expenditures'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mus_pol_comb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Spend'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'us_pol_comb' is not defined"
     ]
    }
   ],
   "source": [
    "# It is difficult to see the distribution because there are outliers\n",
    "\n",
    "us_pol_comb['Spend'].plot(kind='hist', title='Distribution of Expenditures')\n",
    "us_pol_comb['Spend'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we convert the date to DateTime Objects \n",
    "\n",
    "pol_comb[\"StartDate\"] = pd.to_datetime(pol_comb[\"StartDate\"])\n",
    "pol_comb[\"EndDate\"] = pd.to_datetime(pol_comb[\"EndDate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the data, it is difficult to assess the origin of the political ad, which makes it difficult to indicate at what local time it was released\n",
    "# For the purposes of this project, I identified the country origin of where the ads mostly come from\n",
    "\n",
    "pol_comb['CountryCode'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the majority of ads come from the US (53.58%), I decided to focus only on the ads that originate from the US\n",
    "\n",
    "us_pol_comb = pol_comb[pol_comb['CountryCode'] == 'united states'].reset_index().drop(columns = ['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Although there are still different time zones in the US, I decided to settle on Pacific Standard Time in order to standardize all the times\n",
    "# As a result, ads that were published after 9pm PST could potentially be a part of the next day depending on the region, but we are forced to generalize\n",
    "\n",
    "us_pol_comb.loc[:, \"StartDate\"] = us_pol_comb.loc[:, \"StartDate\"].dt.tz_convert('US/PACIFIC')\n",
    "us_pol_comb.loc[:, \"EndDate\"] = us_pol_comb.loc[:, \"EndDate\"].dt.tz_convert('US/PACIFIC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can extract the month, day of week, and hour of when the ads are released\n",
    "\n",
    "us_pol_comb['StartDOW'] = us_pol_comb['StartDate'].apply(lambda x: x.weekday)\n",
    "us_pol_comb['StartMonth'] = us_pol_comb['StartDate'].apply(lambda x: x.month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Weekdays - It is interesting to note that the ads usually are published on Tuesday/Thursdays, but not during the weekend\n",
    "# This is an issue that we want to focus on\n",
    "\n",
    "dayDict = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'}\n",
    "us_pol_comb['StartDOW'].replace(dayDict, inplace = True)\n",
    "us_pol_comb['StartDOW'].value_counts().plot(kind = 'bar', title = 'Number of Ads by Weekday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to focus on our question, it is important we now group the ads in weekdays or weekends\n",
    "\n",
    "us_pol_comb['isWeekday'] = us_pol_comb['StartDOW'].apply(lambda x: True if x not in ['Saturday','Sunday'] else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now visualize a bar chart of the number of ads aggregated by the part of week they are released\n",
    "# We observe a high number of ads released on a weekday compared to the weekend\n",
    "\n",
    "dow = us_pol_comb[['isWeekday', 'Spend']]\n",
    "dow_counts = dow.groupby('isWeekday').count()\n",
    "dow_counts.plot.bar(title = 'Number of Ads Released')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly, we can see that more money was spent on average on ads released on a weekday as compared to on a weekend\n",
    "# However, this visualization can be biased due to outliers in the data\n",
    "\n",
    "dow_median_spend = dow.groupby('isWeekday').mean()\n",
    "dow_median_spend.plot.bar(title = 'Average Amount of Money Spent on Ads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We visualized the distributions of the expenditures on the weekday vs the weekend and noticed most of the data points are centered around zero\n",
    "# The visualization is wide because it is being drawn out by outliers\n",
    "\n",
    "us_pol_comb.groupby('isWeekday')['Spend'].plot(kind='kde', legend=True, title='Distribution of Expenditures')\n",
    "plt.xlim(-12000, 15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To emphasize the influence of the outliers, we created a box plot to see how the data is distributed\n",
    "# The box itself is squished on the far left because there are so many data points around zero and as a result, there are many outliers (especially for weekdays)\n",
    "\n",
    "weekday = dow[dow['isWeekday'] == True]\n",
    "weekend = dow[dow['isWeekday'] == False]\n",
    "sns.boxplot(data=[weekday['Spend'], weekend['Spend']], orient='h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_pol_comb.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To visualize our data without the outliers, we decided to calculate the z-score of all the expenditures \n",
    "# We got rid of the data points that had a z-score greater than 3, which were 16 data points (printed below were their z-scores)\n",
    "\n",
    "\n",
    "no_out = us_pol_comb.copy()\n",
    "z = np.abs(stats.zscore(no_out['Spend']))\n",
    "print(np.where(z > 3))\n",
    "no_out = no_out[z < 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we took out only 16 data points, we do not expect the distribution of counts to change significantly\n",
    "\n",
    "dayDict = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'}\n",
    "no_out['StartDOW'].replace(dayDict, inplace = True)\n",
    "no_out['StartDOW'].value_counts().plot(kind = 'bar', title = 'Number of Ads by Weekday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The distribution around zero \n",
    "\n",
    "no_out.groupby('isWeekday')['Spend'].plot(kind='kde', legend=True, title='Distribution of Expenditures')\n",
    "plt.xlim(-12000, 15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can go back to categorizing the ads by time of week without including the outliers\n",
    "\n",
    "dow_no_out = no_out[['isWeekday', 'Spend']]\n",
    "dow_no_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By taking out the outliers, we can drastically see a change in the average amount of money spent\n",
    "# This is because all the outliers were in weekdays, which means that the companies that invested heavily on their ad wanted it to be released on a weekday\n",
    "\n",
    "dow_median_spend_no_out = dow_no_out.groupby('isWeekday').mean()\n",
    "dow_median_spend_no_out.plot.bar(title = 'Average Amount of Money Spent on Ads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday_outno = dow_no_out[dow_no_out['isWeekday'] == True]\n",
    "weekend_outno = dow_no_out[dow_no_out['isWeekday'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Although not as visual as we would want, the data is distributed as such by the boxplots because the data is not uniformly distributed\n",
    "# There are several data points that are still outliers (not as extreme) and that can be seen in the distributions\n",
    "\n",
    "sns.boxplot(data=[weekday_outno['Spend'], weekend_outno['Spend']], orient='h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dayDict = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'}\n",
    "# us_pol_comb['StartDOW'].replace(dayDict, inplace = True)\n",
    "us_pol_comb\n",
    "pd.pivot_table(us_pol_comb, values = 'Spend', index = 'StartDOW', columns = 'StartMonth', aggfunc = np.sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessment of Missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.662099Z",
     "start_time": "2019-10-31T23:36:28.660016Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation Test - Testing by Simulation\n",
    "- **Null hypothesis**: There is no significant difference between the amount of money spent on ads shown on weekends and weekdays.\n",
    "- **Alternate hypothesis**: There is a significant difference between the amount of money spent on ads shown on weekends and weekdays.\n",
    "- **Test Statistic**: Absolute difference in means\n",
    "\n",
    "set a significance level of 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#observed means\n",
    "means_table = dow.groupby('isWeekday').mean()\n",
    "means_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#observed test statistic\n",
    "observed_difference = means_table.diff().iloc[-1,0]\n",
    "observed_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simulation\n",
    "\n",
    "N = 1000\n",
    "results = []\n",
    "\n",
    "for _ in range(N):\n",
    "    #create shuffled dataframe\n",
    "    s = weekday_and_spend['Weekday'].sample(frac=1, replace=False).reset_index(drop=True)\n",
    "    shuffled = weekday_and_spend.assign(weekday=s)\n",
    "    \n",
    "    #calculate difference of means and add to results array\n",
    "    shuff_means_table = shuffled.groupby('weekday').mean()\n",
    "    results.append(abs(shuff_means_table.diff().iloc[-1,0]))\n",
    "\n",
    "diffs_of_means = pd.Series(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs_of_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pval = (diffs_of_means >= observed_difference).sum() / N\n",
    "pval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "* We cannot reject the null hypothesis that there is no significant difference between the amount of money spent on ads shown on weekdays and weekends\n",
    "\n",
    "## However\n",
    "Our exploratory data analysis showed clear outliers - what would happen if these were removed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#observed means\n",
    "means_table_clean = dow_no_out.groupby('isWeekday').mean()\n",
    "means_table_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#observed test statistic\n",
    "observed_difference_clean = means_table_clean.diff().iloc[-1,0]\n",
    "observed_difference_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simulation\n",
    "\n",
    "N = 1000\n",
    "results = []\n",
    "\n",
    "for _ in range(N):\n",
    "    #create shuffled dataframe\n",
    "    s = weekday_and_spend_clean['Weekday'].sample(frac=1, replace=False).reset_index(drop=True)\n",
    "    shuffled = weekday_and_spend_clean.assign(weekday=s)\n",
    "    \n",
    "    #calculate difference of means and add to results array\n",
    "    shuff_means_table = shuffled.groupby('weekday').mean()\n",
    "    results.append(abs(shuff_means_table.diff().iloc[-1,0]))\n",
    "\n",
    "diffs_of_means_clean = pd.Series(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pval = (diffs_of_means_clean >= observed_difference_clean).sum() / N\n",
    "pval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion Without Outliers\n",
    "- with a p-value of less than 0.05, we reject the null hypothesis that there is no significant difference between the amount of money spent on ads shown on weekdays and weekends\n",
    "- we accept the alternate hypothesis - we have found a **significant difference** between the observed distribution and one created by random chance\n",
    "- the outliers have had a significant effect on the outcome of the test - the outliers themselves merit more analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
